/**
 * è‡ªå®šä¹‰Agent - Custom Agent
 * 
 * æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯çš„Agent
 */

import { Message, AgentType, RequestContext, AgentResponse } from '../types';
import { logger } from '../utils/logger';
import { LLMService } from '../services/llm/service';

export interface CustomAgentConfig {
  id: string;
  name: string;
  prompt: string;
  expertise?: string;
}

export class CustomAgent {
  private agentType: AgentType;
  
  constructor(
    private config: CustomAgentConfig,
    private llmService: LLMService
  ) {
    this.agentType = `CUSTOM_${config.id}` as AgentType;
    logger.info(`Custom agent initialized: ${config.name}`);
  }

  /**
   * æ‰§è¡ŒAgentä»»åŠ¡
   */
  async execute(context: RequestContext): Promise<AgentResponse> {
    logger.info(`Custom agent ${this.config.name} executing task...`);
    
    try {
      // æ„å»ºç³»ç»Ÿæ¶ˆæ¯
      const systemMessage: Message = {
        role: 'system',
        content: this.config.prompt,
      };

      // æ·»åŠ ä¸“ä¸šé¢†åŸŸä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
      if (this.config.expertise) {
        systemMessage.content += `\n\nä¸“ä¸šé¢†åŸŸï¼š${this.config.expertise}`;
      }

      // æ„å»ºæ¶ˆæ¯å†å²
      const messages: Message[] = [
        systemMessage,
        ...context.history,
        { role: 'user', content: context.userInput }
      ];

      // è°ƒç”¨LLMæœåŠ¡
      const result = await this.executeWithStandardProvider(messages, context);

      return {
        agentType: this.agentType,
        content: result,
        intent: 'CHAT',
        metadata: {
          tokensUsed: 0,
          thinkingProcess: `è‡ªå®šä¹‰å·¥ç¨‹å¸ˆ ${this.config.name} å¤„ç†å®Œæˆ`,
          suggestions: [],
        },
        timestamp: Date.now(),
      };
    } catch (error) {
      logger.error(`Custom agent ${this.config.name} execution failed:`, error as Error);
      throw new Error(`è‡ªå®šä¹‰å·¥ç¨‹å¸ˆ ${this.config.name} æ‰§è¡Œå¤±è´¥: ${(error as Error).message}`);
    }
  }

  /**
   * æµå¼æ‰§è¡ŒAgentä»»åŠ¡
   */
  async executeStream(
    context: RequestContext,
    onChunk: (chunk: string) => void,
    onThinking?: (thinking: string) => void
  ): Promise<void> {
    logger.info(`Custom agent ${this.config.name} executing stream task...`);
    
    try {
      // å‘é€æ€è€ƒè¿‡ç¨‹
      onThinking?.(`ğŸ¯ **${this.config.name}**
${this.config.expertise ? `ä¸“ä¸šé¢†åŸŸï¼š${this.config.expertise}` : ''}

ğŸ’¡ **ç³»ç»Ÿæç¤º**
${this.config.prompt.slice(0, 200)}${this.config.prompt.length > 200 ? '...' : ''}`);

      // æ„å»ºç³»ç»Ÿæ¶ˆæ¯
      const systemMessage: Message = {
        role: 'system',
        content: this.config.prompt,
      };

      // æ·»åŠ ä¸“ä¸šé¢†åŸŸä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
      if (this.config.expertise) {
        systemMessage.content += `\n\nä¸“ä¸šé¢†åŸŸï¼š${this.config.expertise}`;
      }

      // æ„å»ºæ¶ˆæ¯å†å²
      const messages: Message[] = [
        systemMessage,
        ...context.history,
        { role: 'user', content: context.userInput }
      ];

      // æµå¼è°ƒç”¨LLMæœåŠ¡
      await this.executeStreamWithStandardProvider(messages, context, onChunk, onThinking);

    } catch (error) {
      logger.error(`Custom agent ${this.config.name} stream execution failed:`, error as Error);
      throw new Error(`è‡ªå®šä¹‰å·¥ç¨‹å¸ˆ ${this.config.name} æµå¼æ‰§è¡Œå¤±è´¥: ${(error as Error).message}`);
    }
  }

  /**
   * ä½¿ç”¨æ ‡å‡†æä¾›å•†æ‰§è¡Œï¼ˆæ”¯æŒæµå¼ï¼‰
   */
  private async executeStreamWithStandardProvider(
    messages: Message[],
    context: RequestContext,
    onChunk: (chunk: string) => void,
    onThinking?: (thinking: string) => void
  ): Promise<void> {
    // ä½¿ç”¨ LLMService çš„æµå¼æ¥å£
    await this.llmService.chatStream(messages, onChunk, {
      model: context.config.model,
      temperature: context.config.temperature,
      maxTokens: context.config.maxTokens,
      topP: context.config.topP,
      reasoningTokens: context.config.reasoningTokens,
    });
  }

  /**
   * ä½¿ç”¨è‡ªå®šä¹‰æä¾›å•†æ‰§è¡Œ
   */
  private async executeWithCustomProvider(
    messages: Message[],
    context: RequestContext
  ): Promise<string> {
    // è¿™é‡Œéœ€è¦é›†æˆè‡ªå®šä¹‰æä¾›å•†é€»è¾‘
    // æš‚æ—¶ä½¿ç”¨æ ‡å‡†æä¾›å•†é€»è¾‘
    return this.executeWithStandardProvider(messages, context);
  }

  /**
   * ä½¿ç”¨æ ‡å‡†æä¾›å•†æ‰§è¡Œ
   */
  private async executeWithStandardProvider(
    messages: Message[],
    context: RequestContext
  ): Promise<string> {
    const response = await this.llmService.chat(messages, {
      model: context.config.model,
      temperature: context.config.temperature,
      maxTokens: context.config.maxTokens,
      topP: context.config.topP,
      reasoningTokens: context.config.reasoningTokens,
    });

    return response;
  }
}